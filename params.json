{
  "name": "fatweb",
  "tagline": "Fairness, Accountability, and Transparency on the World Wide Web",
  "body": "Recent academic and journalistic reviews of online web services have revealed that many systems exhibit subtle biases reflecting historic discrimination.  Examples include racial bias in [search advertising](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2208240), [gender bias in display advertisements](https://arxiv.org/abs/1408.6491), [racial bias in image recognition](http://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/), [racial discrimination in the sharing economy](http://www.hbs.edu/faculty/Pages/item.aspx?num=46073), [gender inequality in Wikipedia](https://arxiv.org/abs/1501.06307), [racial bias in pricing](https://www.propublica.org/article/asians-nearly-twice-as-likely-to-get-higher-price-from-princeton-review), [nationality bias in mapping services](http://www.ccs.neu.edu/home/cbw/static/pdf/maps_www16.pdf), and [racial bias in web-based delivery](http://www.bloomberg.com/graphics/2016-amazon-same-day/).  The list of production systems exhibiting biases continues to grow and may be [endemic to the way models are trained and the data used](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899).  At the same time,  concerns about user autonomy  and fairness [have](https://sites.google.com/site/ethicsofonlineexperimentation/) [been](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2846909) [raised](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2604168) in the context of online experimentation.  [Understanding the reasons](http://www.kdd.org/kdd2016/subtopic/view/why-should-i-trust-you-explaining-the-predictions-of-any-classifier) behind predictions and outcomes of online web services is important in assessing trust, which is fundamental if one plans to take action based on the outcomes.\r\n\r\n\r\n\r\nThese observations often surprise both users and system designers, resulting in a broad questioning of how to make system behavior more transparent.  Indeed, the recent [United States Report on Big Data and Civil Rights](https://www.whitehouse.gov/blog/2016/05/04/big-risks-big-opportunities-intersection-big-data-and-civil-rights) recommends that the academic community accelerate research to mitigate against discrimination and disparate impact in online services .  To date though, there has not been a venue for the study and discussion of problems and solutions with biased algorithms or models in the context of online services.\r\n\r\n\r\nResearch in this area has begun to emerge, in a variety of computer science subdisciplines.  Work can be divided into four groups.  \r\n* _Case studies_, such as those referred to earlier, consist of concentrated quantitative or qualitative analyses of systems for ethically problematic behavior.  \r\n* _Methods research_  consists of studying quantitative approaches to defining and measuring fairness.  This work includes topics such as formally defining metrics and rigorously auditing systems.  \r\n* _Tools_ refer to software platforms designed to detect ethically problematic behavior.  The majority of these methods use simulated users to conduct `reverse A/B' tests on production systems.  These tools often implement techniques developed in methods work and help produce novel case studies.\r\n* _Remedies_ refer to algorithms designed to avoid ethically problematic behavior.  This work often adopts a metric developed in the methods work and designs an algorithm to balance, for example, fairness against revenue or accuracy.  \r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}